# Week 1 Big Data Glossary - Quick Reference

*A comprehensive glossary following the natural learning flow from traditional systems to modern big data architectures*

---

# WEEK 1 GLOSSARY

---

## ğŸ”¥ The Big Data Problem

**5 Vs of Big Data**
- **Volume** â†’ Massive amounts of data that broke traditional databases
- **Variety** â†’ Mix of structured (tables) + unstructured (JSON, logs, images, videos)
- **Velocity** â†’ Real-time data streams flooding in continuously
- **Veracity** â†’ Data quality and trustworthiness challenges
- **Value** â†’ The actual business insight extractable from raw data

**Vertical Scaling** â†’ Adding more CPU/RAM/disk to single machine (like feeding steroids to one athlete)

**Horizontal Scaling** â†’ Using many small machines working together (team of average players)

**Distributed Systems** â†’ Multiple machines sharing workload instead of one super-machine

---

## ğŸ˜ Hadoop Era - The Pioneer

**Hadoop** â†’ First distributed system that said "use 100 small computers instead of 1 big one"

**HDFS (Hadoop Distributed File System)** â†’ Stores huge files by splitting across multiple nodes

**MapReduce** â†’ Parallel batch processing engine (slow, writes everything to disk)

**YARN** â†’ Resource manager that assigns cluster resources to applications

**Hadoop Evolution**
- **2007** â†’ Hadoop 1.0 (HDFS + MapReduce)
- **2012** â†’ Hadoop 2.0 (Added YARN)
- **Now** â†’ Hadoop 3.x (More scalable, container support)

**Hadoop Ecosystem (Legacy Tools)**
- **Sqoop** â†’ Move data between databases and HDFS
- **Hive** â†’ SQL queries on Hadoop data
- **Pig** â†’ Scripting for data processing
- **Oozie** â†’ Workflow scheduler
- **HBase** â†’ NoSQL database on HDFS

**Key Lesson** â†’ Hadoop proved distributed systems work, but was complex and slow

---

## â˜ï¸ Cloud Revolution

**Cloud Computing** â†’ Rent servers worldwide with credit card instead of buying hardware

**Why Cloud Wins**
- **Scalability** â†’ Add/remove resources instantly
- **Agility** â†’ Setup time: Weeks â†’ Minutes
- **Geo-distribution** â†’ Place servers close to users
- **Disaster Recovery** â†’ Built-in redundancy
- **Cost** â†’ CapEx (big upfront) â†’ OpEx (pay-as-you-go)

**Cloud Types**
- **Private** â†’ Own infrastructure (banks, high security)
- **Public** â†’ AWS, Azure, GCP
- **Hybrid** â†’ Mix of private + public

**Modern Cloud Replacements**
- Sqoop â†’ ADF/Glue
- HBase â†’ CosmosDB/DynamoDB
- Oozie â†’ Airflow
- MapReduce â†’ Spark

---

## âš¡ Spark Revolution

**Apache Spark** â†’ Game-changer that keeps data in memory (like chef cooking continuously)

**MapReduce Problem** â†’ Wrote everything to disk after each step (like cleaning kitchen after each cooking step)

**Spark Solution** â†’ In-memory processing = 10-100x faster

**Spark Architecture**
- **Driver** â†’ The brain that plans workflow
- **Cluster Manager** â†’ Assigns resources (YARN, Kubernetes)
- **Executors** â†’ Workers that execute tasks and hold data in memory

**Spark Capabilities** â†’ Batch + Streaming + Machine Learning + Graph processing

**Key Point** â†’ Spark replaced MapReduce, not entire Hadoop (still uses HDFS)

---

## ğŸ—ï¸ Storage Evolution Story

**Database (OLTP)** â†’ The cash counter
- **Purpose** â†’ Live transactions (insert, update, lookup)
- **Schema** â†’ On Write (define structure before data)
- **Data Type** â†’ Structured only
- **Strength** â†’ Fast transactions
- **Weakness** â†’ Not good for historical analysis

**Data Warehouse (OLAP)** â†’ The accountant's office
- **Purpose** â†’ Historical analytics on years of data
- **Schema** â†’ On Write (structured)
- **Data Type** â†’ Structured only
- **Strength** â†’ Optimized for aggregations
- **Weakness** â†’ Rigid, expensive

**Data Lake** â†’ The storage warehouse
- **Purpose** â†’ Store everything raw (receipts, photos, JSON, logs)
- **Schema** â†’ On Read (apply structure when analyzing)
- **Data Type** â†’ All types (structured + unstructured)
- **Strength** â†’ Flexible, cheap storage
- **Weakness** â†’ Can become data swamp without governance

---

## ğŸ”„ Architecture Evolution

**Traditional (On-Prem Hadoop)**
```text
MySQL â†’ Sqoop â†’ HDFS â†’ MapReduce â†’ Hive/HBase
```

**Modern Cloud (Azure)**
```text
Sources â†’ ADF â†’ ADLS â†’ Databricks/Synapse â†’ SQL/CosmosDB
```

**Modern Cloud (AWS)**
```text
Sources â†’ Glue â†’ S3 â†’ Databricks/Athena/Redshift â†’ RDS/DynamoDB
```

---

## ğŸ§± HDFS Deep Dive

**NameNode** â†’ The metadata brain (knows where every block lives)

**DataNodes** â†’ Store actual data blocks

**Block Size** â†’ 128 MB default (balance between parallelism and metadata overhead)

**Replication Factor** â†’ 3 copies of each block for fault tolerance

**Heartbeats** â†’ DataNodes send "I'm alive" signals every 3 seconds

**Self-Healing** â†’ If node dies, HDFS automatically recreates lost blocks

**High Availability**
- **Hadoop 1.x** â†’ NameNode = Single Point of Failure
- **Hadoop 2.x+** â†’ Active + Standby NameNodes with Zookeeper

**Secondary NameNode** â†’ Creates metadata checkpoints (NOT a backup!)

---

## ğŸ‘· Data Engineer's Role

**Core Responsibilities**
- Build scalable, resilient data pipelines
- Understand entire data flow from source to consumption
- Choose right tools for specific problems
- Design fault-tolerant distributed systems

**Key Skills**
- Understand WHY each tool exists, not just WHAT it does
- Make architectural decisions based on scale, speed, cost
- Avoid treating tools as buzzwords - see them as problem solvers

---

## ğŸ¯ Week 1 Mental Models

**Scaling Mindset** â†’ Monolith (one big machine) â†’ Distributed (many small machines)

**Storage Evolution** â†’ Rigid structures â†’ Flexible raw storage â†’ Smart processing

**Processing Evolution** â†’ Disk-based batch â†’ In-memory real-time

**Infrastructure Evolution** â†’ Buy hardware â†’ Rent in cloud

**Tool Selection** â†’ Understand the problem first, then pick the right tool

---

## ğŸ”‘ Key Takeaways

1. **Big Data broke traditional systems** due to volume, variety, velocity
2. **Hadoop pioneered distributed computing** but was slow and complex
3. **Cloud solved infrastructure problems** with elasticity and pay-per-use
4. **Spark revolutionized processing** with in-memory computing
5. **Storage evolved** from rigid databases to flexible data lakes
6. **Modern architectures** are cloud-native, faster, and simpler
7. **Data Engineers** must understand the entire ecosystem flow

---

## ğŸš€ Success Metrics

After mastering Week 1, you should be able to:
- âœ… Explain why traditional databases failed at scale
- âœ… Describe how distributed systems solve big data challenges
- âœ… Compare Hadoop vs Spark vs Cloud solutions
- âœ… Choose appropriate storage (DB vs DWH vs Data Lake) for use cases
- âœ… Understand the evolution story and make architectural decisions

---

*This glossary captures the complete Week 1 journey from problem identification to solution architecture*